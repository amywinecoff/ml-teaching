{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator # needed for integer only on axis\n",
    "from matplotlib.lines import Line2D # for creating the custom legend\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, mean_absolute_error,mean_squared_error \n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When we evaluate a machine learning model, our primary goal is to have a better sense of how the model's performance will (or will not) extrapolate to future cases. In many industry settings, it is possible to first evaluate model's performance on existing data (i.e., \"offline evaluation\"). Provided that the model performance is acceptable, we can then deploy the model into a production environment. We can then evaluate the model's performance in the production setting (i.e., \"online evaluation\") and compare that to the metrics we obtained during the offline evaluation. Ideally, the model's offline accuracy is pretty close to it's online accuracy (though this is [not always the case](http://kops.uni-konstanz.de/bitstream/handle/123456789/31055/Beel_0-285617.pdf?sequence=1)). If not, we can keep updating the model until it achieves the performance we are aiming for.\n",
    "\n",
    "For academic research, we often do not have the opportunity to deploy models into a live environments. Instead, our analyses are constrained to offline evaluations. We then publish results in conference papers and journals. As a result, it is especially important that our evaluation methods are well-thought through since we likely will not have an opportunity to iterate through model variations and update our estimates of model accuracy. \n",
    "\n",
    "In this workshop, I will give an introductory overview of evaluation metrics, paradigms, and pitfalls to help you to construct valid evaluation strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "**Accuracy** is the number of correct predictions divided by the total number of predictions made\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{# Correct Predictions}}{\\text{Total # of Predictions}}$$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Imagine that we are trying to build a model to filter out spam email. The following data contains the text of email messages along with their actual type (i.e., label). \"Ham\" messages constitute real email messages whereas \"spam\" messages are, well, spam.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data from https://github.com/PacktPublishing/Machine-Learning-with-R-Third-Edition\n",
    "spam_ham = pd.read_csv('sms_spam.csv')\n",
    "spam_ham = spam_ham[['type']]\n",
    "spam_ham.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a quick look at the type column, we can see that most messages are ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = spam_ham [['type']].groupby(['type']).size().reset_index(name='outcome_counts')\n",
    "\n",
    "counts_df['percentage'] = counts_df['outcome_counts'] / counts_df['outcome_counts'].sum()\n",
    "\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= np.zeros(spam_ham.size)\n",
    "t.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this data, we can build a very simple classifier, which just predicts the majority class. In other words, the classifier will always predict \"ham.\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "#dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "#Note, by definition, the features are ignored in this model\n",
    "dummy_clf.fit(np.zeros(spam_ham.size), spam_ham[['type']])\n",
    "#dummy_clf.fit(spam_ham[['text']], spam_ham[['type']])\n",
    "dummy_predictions = dummy_clf.predict(np.zeros(spam_ham.size))\n",
    "\n",
    "accuracy = round(accuracy_score(spam_ham[['type']], dummy_predictions),2)\n",
    "print(f'Accuracy of majority class model is: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by overall accuracy alone, we might conclude that the dummy classifier is doing a decent job. But of course, this is is not a good model since it never predicts \"spam,\" which is what we are actually trying to get the model to do correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confusion matrix** is used for classification models. It allows us to break down our results in terms of true positives, true negatives, false positives, and false negatives. This gives us a better sense of the kinds of errors our model is making\n",
    "<br/><br/>\n",
    "\n",
    "|            | Predicted true | Predicted false |\n",
    "|------------|----------------|-----------------|\n",
    "|Actual true | True Positive  | False Negative  |\n",
    "|Actual false| False Positive | True Negative   |\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Accordingly, we can reframe our formula for accuracy as\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "<br/><br/>\n",
    "\n",
    "We can create a confusion matrix from our majority class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(spam_ham[['type']], dummy_predictions)\n",
    "\n",
    "#need to flip so spam is the positive case\n",
    "cm = np.flip(cm)\n",
    "\n",
    "\n",
    "cm_df = pd.DataFrame(cm, \n",
    "               columns=['predicted_spam', 'predicted_ham'], \n",
    "               index = ['actual_spam', 'actual_ham'])\n",
    "\n",
    "\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'll consider \"spam\" the positive case since that is what we are trying to detect. So, cases where the model predicted \"ham\" but the message was actually \"spam\" are false negatives. We have 747 of these cases.\n",
    "\n",
    "Because the dummy classifier always predicts \"ham,\" there are no cases where the model predicted \"spam\" and the actual outcome was \"ham;\" however, if it had, those would constitute false positives.\n",
    "\n",
    "Even though our model performed well in terms of overall accuracy, we can see from the confusion matrix that it performed abysmally in terms of false negatives. In machine learning, overall accuracy provides a poor measure of a model's performance when we have a \"class imbalance,\" meaning one label occurs much more frequently than the other(s). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often with machine learning, we neither want a model that is too conservative in predicting the positive class nor too aggressive. Two metrics that help us assess this are precision and recall\n",
    "\n",
    "<br/><br/>\n",
    "**Precision** captures what proportion of the model's prediction of the positive class actually belong to the positive class \n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Recall** captures the proportion of cases that actually belong to the positive class were predicted as positive by the model. \n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "A precise model will only predict the positive class when the example is very likely to be positive. A model that has high recall will capture a large proportion of the actual positive examples. \n",
    "\n",
    "We will calculate precision and recall for our dummy classifier, but first we need to do some minor modifications on our predictions to make this possible. If we attempt to calculate these metrics from just the majority class predictions, we'll get into some divide by zero issues for precision since we have no spam predictions. \n",
    "\n",
    "I'll demonstrate this below, just to show you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm_df.loc['actual_spam', 'predicted_spam']\n",
    "TN = cm_df.loc['actual_ham', 'predicted_ham']\n",
    "FP = cm_df.loc['actual_ham', 'predicted_spam']\n",
    "FN = cm_df.loc['actual_spam', 'predicted_ham']\n",
    "\n",
    "print (f'Precision is {TP/(TP + FP)}')\n",
    "print (f'Recall is {TP/(TP + FN)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. For illustrative purposes, we'll add a few additional observations to our dataset that will allow us to calculate these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame(['spam', 'spam', 'spam', 'spam', \n",
    "                         'spam', 'spam', 'ham', 'ham'], columns=['type'])\n",
    "\n",
    "\n",
    "new_predictions = np.array(['spam', 'spam', 'spam', 'spam', \n",
    "                                'spam', 'spam', 'spam', 'spam'])\n",
    "\n",
    "new_spam_ham = pd.concat([spam_ham, new_data], ignore_index=True)\n",
    "new_dummy_predictions = np.append(dummy_predictions, new_predictions)\n",
    "\n",
    "new_cm = confusion_matrix(new_spam_ham[['type']], new_dummy_predictions)\n",
    "\n",
    "#need to flip so spam is the positive case\n",
    "new_cm = np.flip(new_cm)\n",
    "\n",
    "\n",
    "new_cm_df = pd.DataFrame(new_cm, \n",
    "               columns=['predicted_spam', 'predicted_ham'], \n",
    "               index = ['actual_spam', 'actual_ham'])\n",
    "\n",
    "\n",
    "\n",
    "new_cm_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new confusion matrix reveals that we have 6 true positives, 4812 true negatives, 2 false positives, and 747 false negatives. With these, we can now calculate precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = new_cm_df.loc['actual_spam', 'predicted_spam']\n",
    "TN = new_cm_df.loc['actual_ham', 'predicted_ham']\n",
    "FP = new_cm_df.loc['actual_ham', 'predicted_spam']\n",
    "FN = new_cm_df.loc['actual_spam', 'predicted_ham']\n",
    "\n",
    "dc_precision = TP/(TP + FP)\n",
    "dc_recall = TP/(TP + FN)\n",
    "\n",
    "print (f'Precision is {dc_precision}')\n",
    "print (f'Recall is {dc_recall}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is better than recall. Why is this? We have designed our dummy classifier so that it almost never predicts \"spam.\" The data we added contained only 8 predictions of \"spam,\" 6 of which were correct (6/8 = 0.75). So, when our dummy model does predict the positive class (i.e., \"spam\"), it does a pretty decent job. It's a reasonably precise model.\n",
    "\n",
    "What our model does not do well is cover all the actual instances of spam in the dataset. This is captured in our recall measure, which is very low. Again, this is because our model is not capable of capturing (i.e., correctly predicting) most of the actual spam examples.\n",
    "\n",
    "Precision and recall can tell us a lot about how our model is performing, but it's nice to have a single summary statistic as well. The F1 Score or F-measure, which combines precision and recall using the harmonic mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 score** is the harmonic mean of precision and recall. \n",
    "\n",
    "\n",
    "$$\\text{F}_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_F1 = 2* ((dc_precision*dc_recall)/(dc_precision + dc_recall))\n",
    "print(f'F1 is: {dc_F1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we can also calculate all these metrics using the metrics functionality of scikit learn. We just need to convert our classes to binary first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spam_ham['pos_case'] = np.where(new_spam_ham['type']== 'spam', 1, 0)\n",
    "new_dummy_predictions_binary = np.where(new_dummy_predictions == \"spam\", 1,0)\n",
    "f1_score(new_spam_ham[['pos_case']], new_dummy_predictions_binary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other metrics for evaluating classification models. Techniques that may be a particular help for visualizing results are precision recall curves or receiver operating characteristic (ROC) curves. You can learn more about these techniques here [INSERT LINK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics\n",
    "\n",
    "So far we have only discussed metrics for evaluating classification models, but of course, there are metrics for evaluating regression models as well. Several metrics that are commonly used include mean absolute error, mean squared error, and root mean squared error. In the formulas below, $y_i$ is the actual value for observation i and $\\hat{y}_i$ is the predicted value for observation i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    " \n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Paradigms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add in explanation of overfitting? https://www.quora.com/What-is-an-intuitive-explanation-of-over-fitting-particularly-with-a-small-sample-set-What-are-you-essentially-doing-by-over-fitting-How-does-the-over-promise-of-a-high-R%C2%B2-low-standard-error-occur/answer/Jessica-Su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have discussed evaluation without diving into which data metrics are calculated on. In machine learning, we train a model on one dataset with the goal of making accurate predictions in another dataset. In other words, we aim to create a model that has good generalization. Our evaluation metrics should ideally give us a good sense of how well our model would perform on new data. We will explore several different strategies for training and evaluating models that have different implications for generalization (some good and some not so good).\n",
    "\n",
    "First, let's load some new data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = pd.read_csv('nba_player_statistics.csv')\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will be using for the next section of the workshop contains statistics for NBA players, drawn from www.basketball-reference.com, albeit in modified form $^{ref num}$. The data reflects players' per-season averages for several key metrics. As an example, if a player averaged 10 points per game in their first season, and 20 points per game in their second season, their per-season average points per game is 15. \n",
    "\n",
    "The per-season average statistics we will be using are the following:\n",
    "\n",
    "* **AST** - Assists\n",
    "* **BLK** - Blocks\n",
    "* **PF** - Personal Fouls\n",
    "* **PTS** - Points\n",
    "* **STL** - Steals\n",
    "* **TOV** - Turnovers\n",
    "\n",
    "Using these statistics, we will use KNN classification to predict players' positions. We already provided an introduction to the KNN algorithm in the first day of the Introduction to ML Series, but as a quick refresher, the overarching idea in KNN is that for each example we want to classify, we take the *k* examples whose features are most similar to the new example, and apply whatever label the majority of them have. To determine which examples are most similar, we calculate the distance between the example we are trying to classify and all other examples in our dataset. We often use the Euclidean distance for this, though some other distances are sometimes used. The following formula captures the Euclidean distance for two points *p* and *q*:\n",
    "\n",
    "$$d\\left( p,q\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( q_{i}-p_{i}\\right)^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from www.basketball-reference.com classifies players positions as point guard (PG), shooting guard (SG), power forward (PF), small forward (SF), and center (C). For simplicity, we will collapse those into three categories: guards, forwards, and centers. We'll map those to numeric values in our dataset such that centers=0, forwards=1, and guards=2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map positions to numbers\n",
    "nba['pos_num'] = nba.position.map({'C':0, 'SF':1, 'PF':1, 'SG':2, 'PG':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the model, we'll be using the `scikit-learn` library. To train a model, we'll need to separate out our feature columns (i.e., the basketball stats) from our labels (i.e., the player's position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature data set\n",
    "X = nba[['ast', 'stl', 'blk', 'tov', 'pf', 'pts']]\n",
    "\n",
    "# create response vector (y)\n",
    "y = nba.pos_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the number of players that fall into each position category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = nba[['pos_num']].groupby(['pos_num']).size().reset_index(name='outcome_counts')\n",
    "count_df['proportion'] = count_df['outcome_counts']/sum(count_df['outcome_counts'])\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the spam and ham data we started with, we have a class imbalance in our labels. It is not nearly as severe, but still worth bearing in mind as we calculate our model performance metrics!\n",
    "\n",
    "Before we begin fitting our model, we need transform our features so that they exist on the same scale. Why is this the case? Well, first, let's have a quick look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature plotting\n",
    "sns.set(style='white',font_scale=1.5, rc={'figure.figsize':(30,20)})\n",
    "X.hist(bins=20,color='darkblue' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of some of the features is much larger than others. When we're using KNN, we classify examples using their distance to other examples. If some features have a much large range than others, they will tend to dominate this distance calculation, minimizing the impact of features with smaller ranges. For our algorithm, we'll make use of the `StandardScaler` class in `scikit-learn`, and it's default scaling method, which is [z-scoring](https://en.wikipedia.org/wiki/Standard_score). Note that in the cell below, scaling has been performed across all of our input data. This is not advisable, but I will come back to that in a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scale = scaler.transform(X)\n",
    "X_scale.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on all the data\n",
    "\n",
    "One method for fitting and evaluating models would be to develop a model that is maximally accurate on our entire dataset and report the accuracy as calculated on all the data. This is known as training accuracy because we are evaluating the model on the same data we used to train the model. Let's look at a few examples. Let's start with a model that uses the top 10 nearest neighbors to classify each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# train the model on the entire dataset\n",
    "knn.fit(X_scale, y)\n",
    "\n",
    "# store the predicted response values\n",
    "y_pred_class = knn.predict(X_scale)\n",
    "\n",
    "print (accuracy_score(y, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of overall training accuracy, this is a decent model. But can we make it even better? Let's try with a smaller k...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# train the model on the entire dataset\n",
    "knn.fit(X_scale, y)\n",
    "\n",
    "# store the predicted response values\n",
    "y_pred_class = knn.predict(X_scale)\n",
    "\n",
    "print (accuracy_score(y, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an EVEN SMALLER k..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# train the model on the entire dataset\n",
    "knn.fit(X_scale, y)\n",
    "\n",
    "# store the predicted response values\n",
    "y_pred_class = knn.predict(X_scale)\n",
    "\n",
    "print (accuracy_score(y, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained PERFECT accuracy. Yay! Just kidding, this isn't actually a good thing. Intuitively, it probably makes sense to you that this is an unrealistically optimistic estimate of our model's future performance. In the case of *k*=1, the model is able to obtain perfect accuracy because when we use every example to fit the model, the nearest neighbor to every example is itself:\n",
    "\n",
    "$$d(p,p)=0$$\n",
    "\n",
    "By fitting our model on all our data and evaluating performance on this same dataset, we have created a model that captures our training data perfectly, but is unlikely to generalize to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a visual representation of this, let's look at three plots that show the relationship between the decision boundaries. A decision boundary, is a surface that separates data points belonging to different class lables.\n",
    "<!-- https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mlxtend.plotting import plot_decision_regions\n",
    "neighbors = 25\n",
    "#clf = SVC(C=100,gamma=0.0001)\n",
    "pca = PCA(n_components = 2)\n",
    "X2 = pca.fit_transform(X_scale)\n",
    "\n",
    "# X2 = X_scale[:, 4:6]\n",
    "knn = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn.fit(X2, y)\n",
    "# plot_decision_regions(X2, y.values, clf=knn, legend=1)\n",
    "\n",
    "# # plt.xlabel(X.columns[0], size=14)\n",
    "# # plt.ylabel(X.columns[1], size=14)\n",
    "# plt.xlabel('PC 1')\n",
    "# plt.ylabel('PC 2')\n",
    "# plt.title(f'KNN Decision Region Boundary, k={neighbors}', size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AMY: Convert so that all this plotting code is imported from a .py file. It's too much to have in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=X2\n",
    "\n",
    "# create the x0, x1 feature\n",
    "x0 = x[:,0]\n",
    "x1 = x[:,1]\n",
    "\n",
    "PAD = 1.0 # how much to \"pad\" around the min/max values. helps in setting bounds of plot\n",
    "H = 0.1 # mesh stepsize\n",
    "PROB_DOT_SCALE = 40 # modifier to scale the probability dots\n",
    "PROB_DOT_SCALE_POWER = 3 # exponential used to increase/decrease size of prob dots\n",
    "TRUE_DOT_SIZE = 50 # size of the true labels\n",
    "\n",
    "x0_min, x0_max = np.round(x0.min())-PAD, np.round(x0.max()+PAD)\n",
    "x1_min, x1_max = np.round(x1.min())-PAD, np.round(x1.max()+PAD)\n",
    "\n",
    "x0_axis_range = np.arange(x0_min,x0_max, H)\n",
    "x1_axis_range = np.arange(x1_min,x1_max, H)\n",
    "\n",
    "xx0, xx1 = np.meshgrid(x0_axis_range, x1_axis_range)\n",
    "\n",
    "# check the shape of the meshgrid\n",
    "print('xx0.shape:', xx0.shape)\n",
    "print('xx1.shape:', xx1.shape)\n",
    "\n",
    "xx = np.reshape(np.stack((xx0.ravel(),xx1.ravel()),axis=1),(-1,2))\n",
    "print('xx.shape:', xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=knn\n",
    "# prediction of all the little dots\n",
    "yy_hat = clf.predict(xx) \n",
    "\n",
    "# probability of each dot beingthe predicted color\n",
    "yy_prob = clf.predict_proba(xx) \n",
    "                               \n",
    "# the size of each probability dot\n",
    "yy_size = np.max(yy_prob, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'size'   : 10}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# make figure\n",
    "plt.style.use('seaborn-whitegrid') # set style because it looks nice\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18,10), dpi=150)\n",
    "\n",
    "# establish colors and colormap\n",
    "#  * color blind colors, from https://bit.ly/3qJ6LYL\n",
    "redish = '#d73027'\n",
    "orangeish = '#fc8d59'\n",
    "yellowish = '#fee090'\n",
    "blueish = '#4575b4'\n",
    "colormap = np.array([redish,blueish,orangeish])\n",
    "\n",
    "# plot all the little dots, position defined by the xx values, color\n",
    "# defined by the knn predictions (yy_hat), and size defined by the \n",
    "# probability of that color (yy_prob)\n",
    "ax.scatter(xx[:,0], xx[:,1], c=colormap[yy_hat], alpha=0.4, \n",
    "           s=PROB_DOT_SCALE*yy_size**PROB_DOT_SCALE_POWER, linewidths=0,)\n",
    "\n",
    "# plot the contours\n",
    "ax.contour(x0_axis_range, x1_axis_range, \n",
    "           np.reshape(yy_hat,(xx0.shape[0],-1)), \n",
    "           levels=3, linewidths=1, \n",
    "           colors=[redish,blueish, blueish,orangeish,])\n",
    "\n",
    "# plot the original x values.\n",
    "# * zorder is 3 so that the dots appear above all the other dots \n",
    "ax.scatter(x[:,0], x[:,1], c=colormap[y], s=TRUE_DOT_SIZE, zorder=3, \n",
    "           linewidths=0.7, edgecolor='k')\n",
    "\n",
    "# create legends\n",
    "x_min, x_max = ax.get_xlim()\n",
    "y_min, y_max = ax.get_ylim()\n",
    "\n",
    "ax.set_ylabel(\"PC1\")\n",
    "ax.set_xlabel(\"PC2\")\n",
    "ax.set_title(f\"Decision Boundary for $k$={neighbors}\")\n",
    "\n",
    "# create class legend\n",
    "# Line2D properties: https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html\n",
    "# about size of scatter plot points: https://stackoverflow.com/a/47403507/9214620\n",
    "legend_class = []\n",
    "for bball_positions, color in zip(['Center', 'Forward', 'Guard'], [blueish, redish, orangeish]):\n",
    "    legend_class.append(Line2D([0], [0], marker='o', label=bball_positions,ls='None',\n",
    "                               markerfacecolor=color, markersize=np.sqrt(TRUE_DOT_SIZE), \n",
    "                               markeredgecolor='k', markeredgewidth=0.7))\n",
    "\n",
    "# iterate over each of the probabilities to create prob legend\n",
    "prob_values = [0.4, 0.6, 0.8, 1.0]\n",
    "legend_prob = []\n",
    "for prob in prob_values:\n",
    "    legend_prob.append(Line2D([0], [0], marker='o', label=prob, ls='None', alpha=0.8,\n",
    "                              markerfacecolor='grey', \n",
    "                              markersize=np.sqrt(PROB_DOT_SCALE*prob**PROB_DOT_SCALE_POWER), \n",
    "                              markeredgecolor='k', markeredgewidth=0))\n",
    "\n",
    "\n",
    "\n",
    "legend1 = ax.legend(handles=legend_class, loc='center', \n",
    "                    bbox_to_anchor=(1.15, 0.35),\n",
    "                    frameon=False, title='class')\n",
    "\n",
    "legend2 = ax.legend(handles=legend_prob, loc='center', \n",
    "                    bbox_to_anchor=(1.15, 0.65),\n",
    "                    frameon=False, title='prob', )\n",
    "\n",
    "ax.add_artist(legend1) # add legend back after it disappears\n",
    "\n",
    "ax.set_yticks(np.arange(x1_min,x1_max, 1)) # I don't like the decimals\n",
    "ax.grid(False) # remove gridlines (inherited from 'seaborn-whitegrid' style)\n",
    "\n",
    "# only use integers for axis tick labels\n",
    "# from: https://stackoverflow.com/a/34880501/9214620\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# remove first ticks from axis labels, for looks\n",
    "# from: https://stackoverflow.com/a/19503828/9214620\n",
    "ax.set_xticks(ax.get_xticks()[1:-1])\n",
    "ax.set_yticks(np.arange(x1_min,x1_max, 1)[1:])\n",
    "\n",
    "# set the aspect ratio to 1, for looks\n",
    "ax.set_aspect(1)\n",
    "\n",
    "# plt.savefig('knn.svg',dpi=300,format='svg', bbox_inches = \"tight\")\n",
    "# plt.savefig('knn.png',dpi=150,bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we need a better way to estimate the out-of-sample performance of our model, both so that we have a (more) realistic estimate of how our model will perform and so that we can choose a reasonable value for *k*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Splits\n",
    "One alternative approach is to partition your dataset into two subsets. You train your model on one subset (ie., the training set) and evaluate your model in the other (i.e., the test set). The model's performance on the test set is referred to as the **testing accuracy** because we are evaluating the model on an (in theory) independent dataset that was not used during model training. For this reason, testing accuracy is usually a better estimate of out-of-sample performance than training accuracy.\n",
    "\n",
    "We can use the built-in functionality in scikit learn to split our data into training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: split X and y into training and testing sets (using random_state for reproducibility)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=98, test_size=0.20)\n",
    "\n",
    "# STEP 2: Create a scaler, fit to the TRAINING data and apply to train and test\n",
    "scaler = StandardScaler()\n",
    "# # Fit only on X_train\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# # Scale both X_train and X_test\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# STEP 2: train the model on the training set (using K=1)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "#AMY ADD IN SCALING\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# STEP 3: test the model on the testing set, and check the accuracy\n",
    "y_pred_class = knn.predict(X_test)\n",
    "print (accuracy_score(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat for different value of k\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_class = knn.predict(X_test)\n",
    "print (accuracy_score(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing testing accuracy with null accuracy\n",
    "Null accuracy is the accuracy that could be achieved by always predicting the most frequent class. It is a benchmark against which you may want to measure your classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1, 50)\n",
    "training_error = []\n",
    "testing_error = []\n",
    "\n",
    "for k in k_range:\n",
    "\n",
    "    # instantiate the model with the current K value\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    # calculate training error\n",
    "    knn.fit(X_scale, y)\n",
    "    y_pred_class = knn.predict(X_scale)\n",
    "    training_accuracy = accuracy_score(y, y_pred_class)\n",
    "    training_error.append(1 - training_accuracy)\n",
    "    \n",
    "    # calculate testing error\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_class = knn.predict(X_test)\n",
    "    testing_accuracy = accuracy_score(y_test, y_pred_class)\n",
    "    testing_error.append(1 - testing_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame of K, training error, and testing error\n",
    "column_dict = {'K': k_range, 'training error':training_error, 'testing error':testing_error}\n",
    "df = pd.DataFrame(column_dict).set_index('K').sort_index(ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the relationship between K (HIGH TO LOW) and TESTING ERROR\n",
    "df.plot(y='testing error')\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Error (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the minimum testing error and the associated K value\n",
    "df.sort_values('testing error', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using KNN on this dataset with these features, the best value for K is likely to be around 12.\n",
    "Given the statistics of an unknown player, we estimate that we would be able to correctly predict his position about 72% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the relationship between K (from k=50 to k=1) and both training and testing error\n",
    "df.plot()\n",
    "plt.gca().invert_xaxis()\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Error (lower is better)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# # Fit only on X_train\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions on out-of-sample data\n",
    "Given the statistics of a (truly) unknown player, how do we predict his position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiate the model with the best known parameters\n",
    "# knn = KNeighborsClassifier(n_neighbors=14)\n",
    "\n",
    "# # re-train the model with X and y (not X_train and y_train) - why?\n",
    "# knn.fit(X, y)\n",
    "\n",
    "# # make a prediction for an out-of-sample observation\n",
    "# #Amy, need to scale features\n",
    "# knn.predict([1, 1, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages of train/test split?**\n",
    "What would happen if the `train_test_split` function had split the data differently? Would we get the same exact results as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different values for random_state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=96)\n",
    "# STEP 2: Create a scaler, fit to the TRAINING data and apply to train and test\n",
    "scaler = StandardScaler()\n",
    "# # Fit only on X_train\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# # Scale both X_train and X_test\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=14)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_class = knn.predict(X_test)\n",
    "print (accuracy_score(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test split is useful because of its flexibility and speed, but testing accuracy is a high-variance estimate of out-of-sample accuracy. K-fold cross-validation overcomes this limitation and provides more reliable estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps for K-fold cross-validation\n",
    "1. Split the dataset into K equal partitions (or \"folds\").\n",
    "2. Use fold 1 as the testing set and the union of the other folds as the training set. \n",
    "3. Calculate testing accuracy.\n",
    "4. Repeat steps 2 and 3 K times, using a different fold as the testing set each time.\n",
    "5. Use the average testing accuracy as the estimate of out-of-sample accuracy.\n",
    "\n",
    "**Diagram of 5-fold cross-validation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](cross_validation_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/complete-guide-to-pythons-cross-validation-with-examples-a9676b5cac12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "# instantiate the model\n",
    "knn = KNeighborsClassifier(n_neighbors=12)\n",
    "scores = cross_val_score(knn, X_scale, y, cv=5)\n",
    "print(f\"accuracy scores per fold are {scores}\")\n",
    "print(\"%0.2f average accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing cross-validation to train/test split\n",
    "\n",
    "**Advantages of cross-validation:**\n",
    "\n",
    "* More accurate estimate of out-of-sample accuracy\n",
    "* More \"efficient\" use of data (every observation is used for both training and testing)\n",
    "\n",
    "**Advantages of train/test split:**\n",
    "\n",
    "* Runs K times faster than K-fold cross-validation\n",
    "* Simpler to examine the detailed results of the testing process\n",
    "\n",
    "\n",
    "**Cross-validation recommendations**\n",
    "* K can be any number, but K=10 is generally recommended provided you have a reasonable amount of data\n",
    "* For classification problems, stratified sampling is recommended for creating the folds\n",
    "* Each response class should be represented with equal proportions in each of the K folds. scikit-learn's `cross_val_score` function does this by default [AMY CONFIRM] \n",
    "\n",
    "### Cross-validation for parameter tuning\n",
    "\n",
    "**Goal**: Select the best tuning parameters (aka \"hyperparameters\") for KNN. In our case, the parameter we are trying to find the best value for is k. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for an optimal value of K for KNN\n",
    "k_range = range(1, 21)\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_scale, y, cv=10, scoring='accuracy')\n",
    "    print(f\"Mean accuracy for k={k} is: {scores.mean()}\")\n",
    "    k_scores.append(scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation for model selection\n",
    "**Goal**: Compare the best KNN model with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross-validation with the best KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=12)\n",
    "print (cross_val_score(knn, X_scale, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross-validation with logistic regression\n",
    "logreg = LogisticRegression()\n",
    "print (cross_val_score(logreg, X_scale, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other options with cross-validation\n",
    "\n",
    "**Repeated cross-validation**\n",
    "\n",
    "Repeat cross-validation multiple times (with different random splits of the data) and average the results. This creates a more reliable estimate of out-of-sample performance by reducing the variance associated with a single trial of cross-validation\n",
    "\n",
    "**Creating a hold-out set**\n",
    "\n",
    "\"Hold out\" a portion of the data before beginning the model building process\n",
    "Locate the best model using cross-validation on the remaining data, and test it using the hold-out set\n",
    "More reliable estimate of out-of-sample performance since hold-out set is truly out-of-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding evaluation pitfalls\n",
    "**Leakage**:\n",
    "\n",
    "\"Data leakage is a spurious relationship between the independent variables and the target variable that arises as an artifact of the data collection, sampling, or pre-processing strategy. Since the spurious relationship won’t be present in the distribution about which scientific claims are made, leakage usually leads to inflated estimates of model performance [Kapoor & Narayanan, 2022](https://arxiv.org/pdf/2207.07048.pdf)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A commonly told cautionary tale about leakage:\n",
    "\n",
    "*Tomaso Poggio⁠, the director of M.I.T.’s Center for Brains, Minds and Machines, offered a classic parable used to illustrate this disconnect. The Army trained a program to differentiate American tanks from Russian tanks with 100% accuracy. Only later did analysts realized that the American tanks had been photographed on a sunny day and the Russian tanks had been photographed on a cloudy day. The computer had learned to detect brightness ([Murphy, 2017](https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](dalle-tanks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bizzare tanks care of [DALL-E](https://labs.openai.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite being widely referenced, there has been [debate](https://www.gwern.net/Tanks) about whether this actually ever happened or even could happen. Still, the urban legend offers an easy to understand example of data leakage--if there is an artifact in the data collection process that is confounded with the outcome/target/response variable, estimates of model accuracy will be inflated compared to a real-world scenario where the confound is not present.\n",
    "\n",
    "Think about this as being similar to experimental design. As much as possible, we want to minimize any differences between our treatment and control groups to ensure that our results have internal validity (and ideally, external validity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some sneaky ways that your choices developing machine learning models can create data leakage. Michael Lones has created an excellent, easy to understand [guide](https://arxiv.org/pdf/2108.02497.pdf) to some of the most common pitfalls. We will only touch on a few today, but you are encouraged to read his guide!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\">\n",
    "\n",
    "**Notes on source material**\n",
    "    \n",
    "* This evaluation paradigms section of the workshop notebook is takes sections, some nearly verbatim, from [this notebook](https://github.com/justmarkham/DAT8/blob/master/notebooks/09_model_evaluation.ipynb) because it contains a great explaination of evaluation paradigms. Some material was also adapted from maykulkarni's machine learning notebooks repo [https://github.com/maykulkarni], and other parts were adapted from [this tutorial](https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/). Parts of the cross-validation section are adapted from [this notebook](https://github.com/justmarkham/DAT8/blob/master/notebooks/13_cross_validation.ipynb) \n",
    "    \n",
    "* The rad decision boundariy plot was adapted from [this post](https://www.tvhahn.com/posts/beautiful-plots-decision-boundary/)\n",
    "\n",
    "* The NBA data for the KNN example was generated using the [basketball reference scraper repo](https://github.com/vishaalagartha/basketball_reference_scraper) to scrape NBA statistics from the website https://www.basketball-reference.com. Source data extracted using this repo were transformed into their current format by adapting code from [this tutorial](https://towardsdatascience.com/simple-modeling-of-nba-positions-using-the-k-nearest-neighbors-machine-learning-algorithm-223b8addb08f) on KNN. You can access my complete code for generating the data [here](https://github.com/amywinecoff/baskeball-scraper/blob/main/get_basketball_data.ipynb). I apologize in advance for the chaos of this repo.\n",
    "</font> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
